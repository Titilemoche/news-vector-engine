# üöÄ news-vector-engine

**`news-vector-engine` est une pipeline d'ingestion et de traitement de donn√©es con√ßue pour agr√©ger des articles d'actualit√©, les enrichir gr√¢ce √† des techniques de Traitement Naturel du Langage (NLP) avanc√©es, les vectoriser et permettre leur interrogation via une base de donn√©es vectorielle.**

L'objectif principal est de fournir un flux de travail automatis√© pour transformer des articles bruts en informations structur√©es et s√©mantiquement riches, pr√™tes √† √™tre explor√©es et analys√©es, notamment par des "personas" ou agents vectoriels sp√©cialis√©s.

Ce projet s'adresse aux d√©veloppeurs, chercheurs et analystes de donn√©es int√©ress√©s par :
- La veille technologique et informationnelle automatis√©e.
- La cr√©ation de bases de connaissances s√©mantiques.
- L'exp√©rimentation avec les mod√®les de langage (LLM) pour l'enrichissement de texte.
- L'utilisation de bases de donn√©es vectorielles pour la recherche s√©mantique.

## üìú Table des Mati√®res

1.  [‚ú® Pr√©sentation du Projet](#-news-vector-engine)
2.  [üìÇ Structure du Projet](#-structure-du-projet)
3.  [üõ†Ô∏è Technologies Utilis√©es](#Ô∏è-technologies-utilis√©es)
4.  [‚öôÔ∏è Architecture Globale](#Ô∏è-architecture-globale)
5.  [üöÄ D√©marrage Rapide (Installation et Lancement)](#-d√©marrage-rapide-installation-et-lancement)
    *   [Pr√©requis](#pr√©requis)
    *   [Installation](#installation)
    *   [Configuration](#configuration)
    *   [Ex√©cution du Pipeline](#ex√©cution-du-pipeline)
6.  [üîÅ D√©tail du Pipeline de Donn√©es](#-d√©tail-du-pipeline-de-donn√©es)
    *   [1. Scraping des Flux RSS](#1-scraping-des-flux-rss)
    *   [2. Enrichissement des Articles](#2-enrichissement-des-articles)
    *   [3. Vectorisation des Articles](#3-vectorisation-des-articles)
    *   [4. Indexation dans Qdrant](#4-indexation-dans-qdrant)
    *   [5. Export pour TensorFlow Projector](#5-export-pour-tensorflow-projector)
7.  [üß† Fonctionnement des Personas](#-fonctionnement-des-personas)
8.  [üîç Interrogation de la Base Vectorielle (Qdrant)](#-interrogation-de-la-base-vectorielle-qdrant)
9.  [üß™ Exemples et R√©sultats (√Ä venir)](#-exemples-et-r√©sultats-√†-venir)
10. [ü§ù Contribuer au Projet](#-contribuer-au-projet)
11. [üìÑ Licence](#-licence)
12. [üôè Remerciements](#-remerciements)

## üìÇ Structure du Projet

Voici l'arborescence principale du projet et la description de chaque r√©pertoire/fichier cl√© :

```
news-vector-engine/
‚îú‚îÄ‚îÄ .env                   # Variables d'environnement (API cl√©s, etc.) - NON VERSIONN√â
‚îú‚îÄ‚îÄ .gitignore             # Fichiers et dossiers ignor√©s par Git
‚îú‚îÄ‚îÄ LICENSE                # Licence du projet (MIT)
‚îú‚îÄ‚îÄ README.md              # Ce fichier
‚îú‚îÄ‚îÄ requirements.txt       # D√©pendances Python du projet
‚îÇ
‚îú‚îÄ‚îÄ config/                # Fichiers de configuration
‚îÇ   ‚îî‚îÄ‚îÄ sources.json       # Configuration des sources de donn√©es (flux RSS par persona)
‚îÇ
‚îú‚îÄ‚îÄ data/                  # Donn√©es g√©n√©r√©es et trait√©es par le pipeline
‚îÇ   ‚îú‚îÄ‚îÄ feeds_raw/         # Articles bruts scrap√©s des flux RSS (JSON par persona)
‚îÇ   ‚îú‚îÄ‚îÄ enriched/          # Articles enrichis par les modules NLP (JSON par persona et par article)
‚îÇ   ‚îú‚îÄ‚îÄ embeddings/        # Articles avec leurs embeddings vectoriels (JSON par persona et par article)
‚îÇ   ‚îî‚îÄ‚îÄ tensorflow_projector/ # Donn√©es export√©es pour TensorFlow Projector (vectors.tsv, metadata.tsv)
‚îÇ
> **Note :** Les sous-r√©pertoires de `data/` (`feeds_raw/`, `enriched/`, `embeddings/`, `tensorflow_projector/`) sont inclus dans le d√©p√¥t avec une structure vide (maintenue par des fichiers `.gitkeep` que nous allons cr√©er). Ils seront remplis avec les donn√©es g√©n√©r√©es uniquement lorsque vous ex√©cuterez les scripts du pipeline. Aucune donn√©e d'article r√©elle n'est versionn√©e dans ces r√©pertoires.
‚îÇ
‚îú‚îÄ‚îÄ docs/                  # Documentation additionnelle, plans d'architecture, sp√©cifications
‚îÇ
‚îú‚îÄ‚îÄ personas/              # Configuration sp√©cifique aux personas (pr√©vu pour √©volutions futures)
‚îÇ   ‚îî‚îÄ‚îÄ personas_config.json # Fichier de configuration d√©taill√© par persona (actuellement vide)
‚îÇ
‚îî‚îÄ‚îÄ pipeline/              # Scripts principaux du pipeline de traitement
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îú‚îÄ‚îÄ scrape_feed.py     # Script de scraping des flux RSS
    ‚îú‚îÄ‚îÄ enrich_articles.py # Script d'orchestration de l'enrichissement NLP
    ‚îú‚îÄ‚îÄ vectorize_articles.py # Script de g√©n√©ration des embeddings vectoriels
    ‚îú‚îÄ‚îÄ ingest_to_qdrant.py # Script d'ingestion des donn√©es dans Qdrant
    ‚îú‚îÄ‚îÄ export_for_tensorflow_projector.py # Script d'export pour TensorFlow Projector
    ‚îú‚îÄ‚îÄ llm_client.py      # Client pour interagir avec les API LLM (Gemini)
    ‚îú‚îÄ‚îÄ prompts.py         # D√©finitions des prompts pour les LLM
    ‚îú‚îÄ‚îÄ qdrant_utils.py    # Fonctions utilitaires pour Qdrant (client, cr√©ation de collection)
    ‚îÇ
    ‚îî‚îÄ‚îÄ nlp_modules/       # Modules sp√©cifiques pour les t√¢ches NLP
        ‚îú‚îÄ‚îÄ __init__.py
        ‚îú‚îÄ‚îÄ cleaner.py     # Module de nettoyage de texte (actuellement placeholder)
        ‚îú‚îÄ‚îÄ summarize.py   # Module de r√©sum√© de texte via LLM
        ‚îú‚îÄ‚îÄ tagger.py      # Module de tagging (actuellement stub, pr√©vu pour LLM)
        ‚îú‚îÄ‚îÄ classifier.py  # Module de classification (actuellement stub)
        ‚îî‚îÄ‚îÄ entities.py    # Module d'extraction d'entit√©s (actuellement stub)
```

## üõ†Ô∏è Technologies Utilis√©es

Ce projet s'appuie sur les technologies et librairies suivantes :

-   **Langage :** Python 3.x
-   **Gestion des d√©pendances :** `pip` et `requirements.txt`
-   **Manipulation de donn√©es :**
    -   `feedparser` : Pour parser les flux RSS.
    -   `json` : Pour la manipulation des fichiers de donn√©es interm√©diaires.
-   **Variables d'environnement :**
    -   `python-dotenv` : Pour charger les configurations depuis le fichier `.env`.
-   **Mod√®les de Langage (LLM) :**
    -   `google-generativeai` : Pour interagir avec l'API Gemini (summarization).
    -   `openai` : Pour g√©n√©rer les embeddings vectoriels (mod√®le `text-embedding-3-small`).
-   **Base de Donn√©es Vectorielle :**
    -   `qdrant-client` : Pour interagir avec la base de donn√©es vectorielle Qdrant (stockage et recherche s√©mantique).
-   **Visualisation (Pr√©vu) :**
    -   TensorFlow Projector : Pour la visualisation 3D des embeddings (via export TSV).
-   **Qualit√© de code et outillage (Recommand√© pour contribution) :**
    -   Linters (e.g., Flake8, Pylint)
    -   Formatters (e.g., Black, isort)

## ‚öôÔ∏è Architecture Globale

Le sch√©ma ci-dessous illustre le flux de donn√©es global du projet `news-vector-engine` :

```mermaid
graph TD
    A[Flux RSS Sources (.json)] --> B(pipeline/scrape_feed.py);
    B --> C[data/feeds_raw/*.json];
    C --> D(pipeline/enrich_articles.py);
    D -- Utilise --> E(pipeline/nlp_modules/*);
    E -- Notamment --> F(pipeline/nlp_modules/summarize.py);
    F -- Appelle --> G(pipeline/llm_client.py);
    G -- Interagit avec --> H[API Google Gemini];
    D --> I[data/enriched/{persona}/*.json];
    I --> J(pipeline/vectorize_articles.py);
    J -- Interagit avec --> K[API OpenAI Embeddings];
    J --> L[data/embeddings/{persona}/*.json];
    L --> M(pipeline/ingest_to_qdrant.py);
    M -- Utilise --> N(pipeline/qdrant_utils.py);
    N -- Interagit avec --> O[Base de Donn√©es Qdrant];
    L --> P(pipeline/export_for_tensorflow_projector.py);
    P --> Q[data/tensorflow_projector/*.tsv];
    Q --> R[Visualisation TensorFlow Projector];

    subgraph "Configuration"
        A
        H
        K
        O
    end

    subgraph "Pipeline de Traitement"
        B
        D
        E
        F
        G
        J
        M
        N
        P
    end

    subgraph "Stockage Interm√©diaire des Donn√©es"
        C
        I
        L
        Q
    end
```

## üöÄ D√©marrage Rapide (Installation et Lancement)

Suivez ces √©tapes pour configurer et lancer le projet en local.

### Pr√©requis

-   Python 3.8 ou sup√©rieur.
-   Un compte Google Cloud avec une cl√© API pour Gemini (pour les r√©sum√©s).
-   Un compte OpenAI avec une cl√© API (pour la g√©n√©ration des embeddings).
-   Une instance Qdrant en cours d'ex√©cution (locale via Docker ou sur Qdrant Cloud).
    -   Pour Docker : `docker run -p 6333:6333 -p 6334:6334 qdrant/qdrant`

### Installation

1.  **Clonez le d√©p√¥t :**
    ```bash
    git clone <URL_DU_DEPOT_GIT>
    cd news-vector-engine
    ```

2.  **Cr√©ez et activez un environnement virtuel (recommand√©) :**
    ```bash
    python -m venv venv
    # Sur macOS/Linux
    source venv/bin/activate
    # Sur Windows
    .\venv\Scripts\activate
    ```

3.  **Installez les d√©pendances :**
    ```bash
    pip install -r requirements.txt
    ```

### Configuration

1.  **Un fichier `.env.example` est fourni √† la racine du projet.** Copiez-le et renommez la copie en `.env`.
2.  **Ajoutez vos cl√©s API et configurations Qdrant dans le fichier `.env` :**

    ```env
    # Cl√© API pour Google Gemini (utilis√©e par pipeline/llm_client.py)
    GEMINI_API_KEY="VOTRE_CLE_API_GOOGLE_GEMINI"
    # ou GOOGLE_API_KEY="VOTRE_CLE_API_GOOGLE_GEMINI"

    # Cl√© API pour OpenAI (utilis√©e par pipeline/vectorize_articles.py)
    OPENAI_API_KEY="VOTRE_CLE_API_OPENAI"

    # Configuration Qdrant (utilis√©e par pipeline/qdrant_utils.py)
    # Pour une instance Qdrant locale (par d√©faut si non sp√©cifi√© pour le cloud)
    QDRANT_HOST="localhost"
    QDRANT_PORT=6333

    # Pour Qdrant Cloud (d√©commentez et adaptez si besoin)
    # QDRANT_HOST="VOTRE_URL_CLUSTER_QDRANT"
    # QDRANT_PORT=6333 # ou 6334 pour gRPC/HTTPS
    # QDRANT_API_KEY="VOTRE_CLE_API_QDRANT" # Si authentification activ√©e
    ```

    Assurez-vous que votre instance Qdrant est accessible avec ces param√®tres.

### Ex√©cution du Pipeline

Les scripts du pipeline peuvent √™tre ex√©cut√©s s√©quentiellement. Chaque script effectue une √©tape sp√©cifique du traitement des donn√©es.

1.  **Scraping des flux RSS :**
    R√©cup√®re les articles bruts depuis les sources d√©finies dans [`config/sources.json`](config/sources.json:1) et les sauvegarde dans [`data/feeds_raw/`](data/feeds_raw/.gitkeep:1).
    ```bash
    python pipeline/scrape_feed.py
    ```

2.  **Enrichissement des articles :**
    Traite les articles bruts, applique les modules NLP (nettoyage, r√©sum√© LLM, tagging stub, etc.) et sauvegarde les r√©sultats dans [`data/enriched/`](data/enriched/persona_ai_builder/_Liger_GRPO_meets_TRL.json:1).
    ```bash
    python pipeline/enrich_articles.py
    ```

3.  **Vectorisation des articles (Cr√©ation des Embeddings) :**
    G√©n√®re les embeddings vectoriels pour les articles enrichis en utilisant l'API OpenAI et les sauvegarde dans [`data/embeddings/`](data/embeddings/.gitkeep:1).
    ```bash
    python pipeline/vectorize_articles.py
    ```

4.  **Indexation dans Qdrant :**
    Ing√®re les articles (avec leurs embeddings et m√©tadonn√©es) dans la collection Qdrant sp√©cifi√©e.
    ```bash
    python pipeline/ingest_to_qdrant.py
    ```

5.  **Export pour TensorFlow Projector (Optionnel) :**
    Exporte les vecteurs et m√©tadonn√©es au format TSV pour visualisation avec TensorFlow Projector. Les fichiers sont sauvegard√©s dans [`data/tensorflow_projector/`](data/tensorflow_projector/metadata.tsv:1).
    ```bash
    python pipeline/export_for_tensorflow_projector.py
    ```

Chaque script utilise le module `logging` pour afficher sa progression et les √©ventuels probl√®mes dans la console.

## üîÅ D√©tail du Pipeline de Donn√©es

### 1. Scraping des Flux RSS

-   **Script :** [`pipeline/scrape_feed.py`](pipeline/scrape_feed.py:1)
-   **Entr√©e :** Configuration des sources dans [`config/sources.json`](config/sources.json:1).
    ```json
    [
      {
        "persona_id": "persona_ai_builder",
        "source_name": "Hugging Face Blog",
        "type": "rss",
        "url": "https://huggingface.co/blog/feed.xml"
      }
      // ... autres sources
    ]
    ```
-   **Processus :** Utilise `feedparser` pour lire chaque URL de flux RSS. Extrait le titre, le lien, la date de publication, le r√©sum√© brut, et associe le `persona_id` et `source_name`.
-   **Sortie :** Fichiers JSON (un par `persona_id`) dans [`data/feeds_raw/`](data/feeds_raw/.gitkeep:1), contenant une liste d'articles bruts.

### 2. Enrichissement des Articles

-   **Script :** [`pipeline/enrich_articles.py`](pipeline/enrich_articles.py:1)
-   **Entr√©e :** Fichiers JSON de [`data/feeds_raw/`](data/feeds_raw/.gitkeep:1).
-   **Processus :**
    1.  Pour chaque article, combine le titre et le r√©sum√© brut.
    2.  **Nettoyage (`pipeline/nlp_modules/cleaner.py`) :** Actuellement un placeholder, retourne le texte original. Pr√©vu pour des op√©rations comme la suppression de HTML, la normalisation, etc.
    3.  **Tagging (`pipeline/nlp_modules/tagger.py`) :** Actuellement un stub retournant des tags statiques. Un prompt avanc√© (`PROMPT_TAGGING_ADVANCED_EN` dans [`pipeline/prompts.py`](pipeline/prompts.py:1)) est d√©fini pour une future impl√©mentation LLM.
    4.  **Classification (`pipeline/nlp_modules/classifier.py`) :** Actuellement un stub retournant une cat√©gorie statique.
    5.  **Extraction d'Entit√©s (`pipeline/nlp_modules/entities.py`) :** Actuellement un stub retournant des entit√©s statiques.
    6.  **R√©sum√© LLM (`pipeline/nlp_modules/summarize.py`) :** Utilise `call_gemini` (via [`pipeline/llm_client.py`](pipeline/llm_client.py:1)) et `PROMPT_SUMMARY` pour g√©n√©rer un r√©sum√© concis.
    7.  Cr√©e un champ `text_for_embedding` combinant le titre, le r√©sum√© enrichi et les tags.
-   **Sortie :** Fichiers JSON individuels pour chaque article enrichi dans [`data/enriched/{persona_name}/`](data/enriched/persona_ai_builder/_Liger_GRPO_meets_TRL.json:1). Chaque fichier contient l'article original enrichi avec les nouvelles m√©tadonn√©es (r√©sum√© LLM, tags, cat√©gorie, entit√©s, texte pour embedding).

### 3. Vectorisation des Articles

-   **Script :** [`pipeline/vectorize_articles.py`](pipeline/vectorize_articles.py:1)
-   **Entr√©e :** Fichiers JSON d'articles enrichis de [`data/enriched/`](data/enriched/persona_ai_builder/_Liger_GRPO_meets_TRL.json:1).
-   **Processus :**
    1.  Pour chaque article, r√©cup√®re le champ `text_for_embedding`.
    2.  Appelle l'API OpenAI Embeddings (mod√®le `text-embedding-3-small`, configurable) pour g√©n√©rer un vecteur d'embedding.
    3.  Ajoute le vecteur d'embedding √† l'objet JSON de l'article.
-   **Sortie :** Fichiers JSON (un par article) dans [`data/embeddings/{persona_name}/`](data/embeddings/.gitkeep:1), contenant les articles enrichis plus leur vecteur d'embedding.

### 4. Indexation dans Qdrant

-   **Script :** [`pipeline/ingest_to_qdrant.py`](pipeline/ingest_to_qdrant.py:1)
-   **Utilitaires :** [`pipeline/qdrant_utils.py`](pipeline/qdrant_utils.py:1)
-   **Entr√©e :** Fichiers JSON avec embeddings de [`data/embeddings/`](data/embeddings/.gitkeep:1).
-   **Processus :**
    1.  Se connecte au client Qdrant (configur√© via `.env`).
    2.  S'assure que la collection (`news_vectors`, taille de vecteur 1536, distance Cosine) existe et cr√©e les index de payload n√©cessaires (pour `source_persona`, `content_type`, `category`, `tags`, `published`) si ce n'est pas d√©j√† fait.
    3.  Pour chaque article :
        -   G√©n√®re un ID de point stable (UUIDv5 bas√© sur le lien de l'article).
        -   Pr√©pare le payload (toutes les m√©tadonn√©es de l'article sauf l'embedding). La date de publication est normalis√©e au format ISO 8601. Les tags sont assur√©s d'√™tre une liste de cha√Ænes.
        -   Ing√®re les points (ID, vecteur, payload) dans Qdrant par lots.
-   **Sortie :** Donn√©es index√©es dans la collection Qdrant.

### 5. Export pour TensorFlow Projector

-   **Script :** [`pipeline/export_for_tensorflow_projector.py`](pipeline/export_for_tensorflow_projector.py:1)
-   **Entr√©e :** Fichiers JSON avec embeddings de [`data/embeddings/`](data/embeddings/.gitkeep:1).
-   **Processus :**
    1.  Cr√©e deux fichiers TSV : `vectors.tsv` et `metadata.tsv`.
    2.  `metadata.tsv` inclut un en-t√™te d√©fini (`title`, `tags`, `published`, `persona`, `link`, `category`, `enriched_summary`).
    3.  Pour chaque article, √©crit son vecteur dans `vectors.tsv` et ses m√©tadonn√©es correspondantes dans `metadata.tsv`. Les valeurs sont nettoy√©es pour le format TSV.
-   **Sortie :** Fichiers `vectors.tsv` et `metadata.tsv` dans [`data/tensorflow_projector/`](data/tensorflow_projector/metadata.tsv:1).

## üß† Fonctionnement des Personas

Dans ce projet, un "persona" repr√©sente un profil d'int√©r√™t ou un agent vectoriel sp√©cialis√©. Actuellement, les personas sont principalement utilis√©s pour :

1.  **Organiser les sources de donn√©es :** Dans [`config/sources.json`](config/sources.json:1), chaque flux RSS est associ√© √† un `persona_id`. Cela permet de collecter des articles pertinents pour des th√®mes ou des perspectives sp√©cifiques (par exemple, "AI Builder", "AI Creator", "AI Investor").
2.  **Segmenter les donn√©es trait√©es :** Les donn√©es brutes, enrichies et vectoris√©es sont stock√©es dans des sous-r√©pertoires nomm√©s d'apr√®s le `persona_id` (e.g., `data/feeds_raw/persona_ai_builder.json`).
3.  **Filtrage potentiel dans Qdrant :** Le champ `source_persona` est index√© dans Qdrant, ce qui permettra √† l'avenir de filtrer les recherches vectorielles par persona.

**√âvolutions futures pour les personas :**

-   Le fichier [`personas/personas_config.json`](personas/personas_config.json:1) (actuellement vide) est pr√©vu pour d√©finir des configurations plus fines par persona, telles que :
    -   Des mots-cl√©s sp√©cifiques pour affiner le filtrage ou le scoring des articles.
    -   Des prompts LLM personnalis√©s pour le r√©sum√© ou le tagging, adapt√©s au style ou aux besoins du persona.
    -   Des strat√©gies de requ√™tage sp√©cifiques pour la base vectorielle.

L'id√©e est que chaque persona puisse interagir avec la base de connaissances vectorielle d'une mani√®re qui lui est propre, en se concentrant sur les informations les plus pertinentes pour son domaine d'expertise ou d'int√©r√™t.

## üîç Interrogation de la Base Vectorielle (Qdrant)

Une fois les articles vectoris√©s et index√©s dans Qdrant, vous pouvez effectuer des recherches s√©mantiques pour trouver des articles similaires √† une requ√™te donn√©e ou explorer les donn√©es.

Actuellement, le projet ne fournit pas de script d√©di√© pour l'interrogation, mais cela peut √™tre fait en utilisant le client Qdrant en Python. Voici un exemple conceptuel de la mani√®re dont vous pourriez interroger la base :

```python
# Exemple conceptuel de requ√™te Qdrant (√† adapter dans un script d√©di√©)
from qdrant_client import QdrantClient, models # Ajout de models pour Filter
from openai import OpenAI # Pour g√©n√©rer l'embedding de la requ√™te

# Charger les configurations (API Keys, Qdrant host)
# ... (similaire √† qdrant_utils.py ou vectorize_articles.py)

# Initialiser les clients
qdrant_client = QdrantClient(host="localhost", port=6333) # ou config cloud
openai_client = OpenAI(api_key="VOTRE_CLE_API_OPENAI")

COLLECTION_NAME = "news_vectors"
OPENAI_EMBEDDING_MODEL = "text-embedding-3-small"

def get_query_embedding(text: str):
    response = openai_client.embeddings.create(input=[text], model=OPENAI_EMBEDDING_MODEL)
    return response.data[0].embedding

# Votre requ√™te de recherche
query_text = "Quelles sont les derni√®res avanc√©es en mati√®re de mod√®les de langage multimodaux ?"
query_vector = get_query_embedding(query_text)

# Effectuer la recherche dans Qdrant
search_results = qdrant_client.search(
    collection_name=COLLECTION_NAME,
    query_vector=query_vector,
    limit=5,  # Nombre de r√©sultats √† retourner
    # Optional: query_filter pour filtrer par persona, date, etc.
    # query_filter=models.Filter(
    #     must=[
    #         models.FieldCondition(key="source_persona", match=models.MatchValue(value="persona_ai_builder"))
    #     ]
    # )
    with_payload=True # Pour r√©cup√©rer les m√©tadonn√©es des articles
)

for hit in search_results:
    print(f"Article ID: {hit.id}, Score: {hit.score}")
    print(f"Titre: {hit.payload.get('title')}")
    print(f"Lien: {hit.payload.get('link')}")
    print(f"R√©sum√© Enrichi: {hit.payload.get('enriched_summary')}")
    print("-" * 20)
```

**Points cl√©s pour l'interrogation :**

1.  **Vectorisation de la requ√™te :** Votre texte de requ√™te doit √™tre transform√© en un vecteur d'embedding en utilisant le m√™me mod√®le que celui utilis√© pour les articles (ici, `text-embedding-3-small` d'OpenAI).
2.  **Utilisation de `qdrant_client.search()` :** Cette fonction prend le vecteur de requ√™te et retourne les points les plus similaires de la collection.
3.  **Filtrage (Optionnel) :** Vous pouvez utiliser des filtres pour affiner les r√©sultats en fonction des m√©tadonn√©es (e.g., `source_persona`, `category`, `published` date).
4.  **`with_payload=True` :** Pour r√©cup√©rer les informations stock√©es dans le payload de chaque point (titre, r√©sum√©, etc.).

Un script d√©di√© ou une petite API (par exemple avec FastAPI ou Flask) pourrait √™tre d√©velopp√© pour faciliter ces interrogations.

## üß™ Exemples et R√©sultats (√Ä venir)

Cette section sera compl√©t√©e avec :

-   Des exemples concrets de requ√™tes et les r√©sultats obtenus.
-   Des captures d'√©cran ou des liens vers des visualisations TensorFlow Projector.
-   Des analyses ou des insights tir√©s des donn√©es trait√©es.

## ü§ù Contribuer au Projet

Les contributions sont les bienvenues ! Si vous souhaitez am√©liorer ce projet, voici quelques pistes :

-   **Impl√©menter les modules NLP manquants :** Remplacer les stubs de `cleaner`, `tagger`, `classifier`, `entities` par des logiques fonctionnelles (bas√©es sur des r√®gles, du ML classique, ou des appels LLM avec les prompts avanc√©s).
-   **D√©velopper une interface de requ√™te :** Cr√©er un script ou une API simple pour interroger la base Qdrant.
-   **Am√©liorer la configuration des personas :** Exploiter [`personas/personas_config.json`](personas/personas_config.json:1) pour une personnalisation plus pouss√©e.
-   **Ajouter des tests unitaires et d'int√©gration.**
-   **Am√©liorer la gestion des erreurs et le monitoring du pipeline.**
-   **√âtendre √† d'autres sources de donn√©es.**
-   **Optimiser les performances.**

**Processus de contribution :**

1.  Forkez le d√©p√¥t.
2.  Cr√©ez une nouvelle branche pour votre fonctionnalit√© ou correction (`git checkout -b feature/ma-nouvelle-feature`).
3.  Effectuez vos modifications et commitez-les (`git commit -m 'Ajout de ma nouvelle feature'`).
4.  Poussez votre branche (`git push origin feature/ma-nouvelle-feature`).
5.  Ouvrez une Pull Request en expliquant clairement vos changements.

Veuillez vous assurer que votre code respecte les standards de qualit√© et inclut la documentation n√©cessaire.

## üìÑ Licence

Ce projet est distribu√© sous la licence MIT. Voir le fichier [`LICENSE`](LICENSE:1) pour plus de d√©tails.

## üôè Remerciements

-   Aux cr√©ateurs et mainteneurs des librairies open source utilis√©es dans ce projet.
-   √Ä la communaut√© pour ses inspirations et ses outils.

---

N'h√©sitez pas √† ouvrir une issue si vous rencontrez des probl√®mes ou si vous avez des suggestions d'am√©lioration !